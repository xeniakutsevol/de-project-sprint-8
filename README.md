# Проектная работа по потоковой обработке с Kafka и Spark Streaming

### Описание
В проекте реализован сервис агрегатора доставки, который:
1. Читает данные об акциях из **Kafka** с помощью **Spark Structured Streaming** и **Python** в режиме реального времени. (Преобразование JSON в датафрейм, Watermark-фильтрация)
2. Получает список подписчиков из базы данных **Postgres**.
3. Джойнит данные из Kafka с данными из БД. (Join потоковых и статичных данных)
4. Отправляет выходное сообщение в Kafka с информацией об акции, пользователе со списком избранного и ресторане, а также вставляет данные в Postgres, чтобы впоследствии получить фидбэк от пользователя. (Использование foreachBatch)

### Как работать с репозиторием
1. Задайте параметры конфигурации в файле `.env`.
